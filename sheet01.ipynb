{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe6dd57c-d551-4162-8bf0-ee1fa6461385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4adbcead-209b-429e-9e50-79a0ab732ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus():\n",
    "    with open(\"./corpus.txt\", \"r\") as corpus:\n",
    "        for line in corpus:\n",
    "            line = line.rstrip()\n",
    "            if line != \"\":\n",
    "                yield line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e6504-cc56-4f89-9978-9e48059c75e2",
   "metadata": {},
   "source": [
    "Task 1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc675751-e06b-4575-a89c-76fa01934d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:  0 :  ['<s>', 'the', 'relative', 'geographical', 'isolation', 'of', 'the', 'brandywine', 'population', 'makes', 'for', 'a', 'limited', 'choice', 'in', 'mating', '</s>']\n",
      "sentence:  1 :  ['<s>', 'then', 'there', 'was', 'a', 'bad', 'delay', 'in', 'getting', 'mort', \"lindsey's\", '30-piece', 'orchestra', 'wedged', 'into', 'its', 'chairs', '</s>']\n",
      "sentence:  2 :  ['<s>', 'if', 'he', 'instructs', 'them', 'in', 'how', 'to', 'evaluate', 'a', 'work', 'he', 'is', 'helping', 'them', 'to', 'achieve', 'their', 'own', 'identity', '</s>']\n",
      "sentence:  3 :  ['<s>', 'all', 'your', 'wishful', 'thinking', \"won't\", 'change', 'that', '</s>']\n",
      "sentence:  4 :  ['<s>', 'indeed', 'the', 'contrary', 'has', 'happened', '</s>']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentence():\n",
    "  for i,sentence in enumerate(load_corpus()):\n",
    "      sentence = re.sub(r\"[,.;:]\", \"\", sentence)\n",
    "      words = re.split(r\"\\s+\", sentence)\n",
    "      words.insert(0, \"<s>\")\n",
    "      words.append(\"</s>\")\n",
    "      yield words\n",
    "for i,sentence in enumerate(tokenize_sentence()):\n",
    "        if i<5:\n",
    "            print(\"sentence: \", i,\": \",sentence)\n",
    "        else: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9972c955-699f-428b-a479-f360480508cd",
   "metadata": {},
   "source": [
    "Task 1b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73c76c28-7347-41ac-a097-0f744159245f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generation': 0.003105590062111801, 'different': 0.003105590062111801, 'to': 0.018633540372670808, 'crew': 0.003105590062111801, 'emotion': 0.003105590062111801, 'information': 0.006211180124223602, \"it's\": 0.003105590062111801, 'at': 0.018633540372670808, 'was': 0.003105590062111801, 'difficulties': 0.003105590062111801, 'there': 0.003105590062111801, 'my': 0.003105590062111801, 'in': 0.06521739130434782, 'themselves': 0.009316770186335404, 'yankees': 0.003105590062111801, 'trained': 0.003105590062111801, 'enemies': 0.003105590062111801, 'level': 0.012422360248447204, 'conditions': 0.006211180124223602, 'artless': 0.003105590062111801, 'and': 0.049689440993788817, 'last': 0.003105590062111801, 'simultaneously': 0.003105590062111801, 'their': 0.003105590062111801, 'as': 0.003105590062111801, 'all': 0.006211180124223602, 'era': 0.003105590062111801, 'sufficient': 0.003105590062111801, 'widespread': 0.003105590062111801, 'project': 0.003105590062111801, 'pool': 0.003105590062111801, 'data': 0.003105590062111801, 'production': 0.006211180124223602, 'scene': 0.003105590062111801, 'strengths': 0.003105590062111801, 'estimates': 0.003105590062111801, 'functionally': 0.003105590062111801, 'cranston': 0.003105590062111801, 'time': 0.021739130434782608, 'undeniable': 0.003105590062111801, '</s>': 0.08695652173913043, 'bridge': 0.003105590062111801, 'a': 0.018633540372670808, 'this': 0.006211180124223602, 'briefly': 0.003105590062111801, 'ambitions': 0.003105590062111801, 'series': 0.006211180124223602, 'interest': 0.006211180124223602, 'are': 0.006211180124223602, 'entitled': 0.003105590062111801, 'volume': 0.003105590062111801, 'edition': 0.015527950310559006, 'weapons': 0.003105590062111801, 'juncture': 0.003105590062111801, 'flood': 0.003105590062111801, 'procedures': 0.003105590062111801, 'yesterday': 0.003105590062111801, 'gaspee': 0.003105590062111801, 'situation': 0.006211180124223602, 'study': 0.009316770186335404, 'republican': 0.006211180124223602, 'blister': 0.003105590062111801, 'the': 0.024844720496894408, 'federal': 0.003105590062111801, 'he': 0.003105590062111801, 'value': 0.003105590062111801, 'stage': 0.003105590062111801, 'hypothesis': 0.003105590062111801, 'council': 0.003105590062111801, 'shape': 0.006211180124223602, 'group': 0.006211180124223602, 'from': 0.003105590062111801, 'maximum': 0.003105590062111801, 'treasonous': 0.003105590062111801, 'within': 0.003105590062111801, 'religious': 0.003105590062111801, 'moral': 0.003105590062111801, 'chapter': 0.003105590062111801, 'book': 0.003105590062111801, 'grief': 0.003105590062111801, 'century': 0.012422360248447204, 'world': 0.006211180124223602, 'session': 0.006211180124223602, 'writer': 0.003105590062111801, 'proceeding': 0.006211180124223602, 'juniors': 0.003105590062111801, 'usage': 0.003105590062111801, 'day': 0.006211180124223602, 'certain': 0.003105590062111801, 'post': 0.006211180124223602, 'i': 0.003105590062111801, 'condition': 0.003105590062111801, 'moment': 0.003105590062111801, 'observations': 0.003105590062111801, 'fear': 0.003105590062111801, 'curves': 0.003105590062111801, '(de': 0.003105590062111801, 'secretary': 0.009316770186335404, 'nashville': 0.003105590062111801, 'company': 0.003105590062111801, 'writers': 0.003105590062111801, 'it': 0.012422360248447204, 'but': 0.006211180124223602, 'lack': 0.006211180124223602, 'members': 0.003105590062111801, 'denies': 0.003105590062111801, 'much': 0.003105590062111801, 'crusade': 0.003105590062111801, 'volume)': 0.003105590062111801, 'action': 0.003105590062111801, 'significant': 0.003105590062111801, 'exhibit)': 0.003105590062111801, 'institutions': 0.003105590062111801, 'be': 0.003105590062111801, 'held': 0.003105590062111801, 'many': 0.003105590062111801, 'stereotype': 0.003105590062111801, 'although': 0.003105590062111801, 'measurements': 0.003105590062111801, 'scope': 0.003105590062111801, 'history': 0.003105590062111801, 'evidence': 0.006211180124223602, 'toward': 0.003105590062111801, 'purposes': 0.003105590062111801, '55000': 0.003105590062111801, 'city': 0.009316770186335404, 'status': 0.006211180124223602, 'an': 0.003105590062111801, '(especially': 0.003105590062111801, 'mood': 0.003105590062111801, 'anthology': 0.003105590062111801, 'will': 0.003105590062111801, 'leadership': 0.003105590062111801, 'studies': 0.003105590062111801, 'term': 0.003105590062111801, 'between': 0.003105590062111801, 'expression': 0.003105590062111801, 'selection': 0.003105590062111801, 'no': 0.003105590062111801, 'bennington': 0.003105590062111801, 'today': 0.003105590062111801, 'salary': 0.003105590062111801, 'premium': 0.003105590062111801, 'occasion': 0.003105590062111801, 'simultaneous': 0.003105590062111801, 'grimm': 0.003105590062111801, 'issue': 0.003105590062111801, 'metaphysical': 0.003105590062111801, 'conflict': 0.003105590062111801, 'paces': 0.003105590062111801, 'great': 0.003105590062111801, 'possibly': 0.003105590062111801, 'plans': 0.003105590062111801, 'capacity': 0.003105590062111801, 'selves': 0.003105590062111801, 'both': 0.003105590062111801, 'conservation': 0.003105590062111801, 'due': 0.003105590062111801, 'accomplishments': 0.003105590062111801, 'contention': 0.003105590062111801, 'work': 0.003105590062111801, 'management': 0.003105590062111801, 'fears': 0.003105590062111801, 'knowledge': 0.003105590062111801, 'blight': 0.003105590062111801, 'played': 0.003105590062111801, 'missiles': 0.003105590062111801, 'technology': 0.003105590062111801, 'ground': 0.003105590062111801, 'police': 0.003105590062111801, 'school': 0.003105590062111801, 'help': 0.003105590062111801, 'when': 0.003105590062111801, 'timetable': 0.003105590062111801, 'agreement': 0.003105590062111801, 'law': 0.003105590062111801, 'plant': 0.003105590062111801, 'were': 0.003105590062111801, 'shortage': 0.003105590062111801, 'scientific': 0.003105590062111801, 'terms??': 0.003105590062111801, 'agitation': 0.003105590062111801, 'by': 0.003105590062111801, 'crop': 0.003105590062111801, 'context': 0.003105590062111801, 'activities': 0.003105590062111801, 'daytime': 0.003105590062111801, 'form': 0.003105590062111801, 'treasurer': 0.003105590062111801, 'opportunities': 0.003105590062111801}\n",
      "{'of': 0.3333333333333333, 'for': 0.3333333333333333, 'which': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "class LanguageModel:\n",
    "  def __init__(self, corpus):\n",
    "    self.unigram_counts = collections.Counter()\n",
    "    self.bigram_counts = collections.Counter()\n",
    "    self.trigram_counts = collections.Counter()\n",
    "\n",
    "    for _, sentence in enumerate(corpus()):\n",
    "      self.unigram_counts.update(sentence)\n",
    "      for i in range(len(sentence) - 1):\n",
    "        self.bigram_counts[(sentence[i], sentence[i + 1])] += 1\n",
    "      for i in range(len(sentence) - 2):\n",
    "        self.trigram_counts[(sentence[i], sentence[i + 1], sentence[i + 2])] += 1\n",
    "\n",
    "    self.total_word_count = sum(self.unigram_counts.values())\n",
    "\n",
    "  def unigram_distribution(self):\n",
    "    unigram_probabilities = {}\n",
    "    for word, count in self.unigram_counts.items():\n",
    "      unigram_probabilities[word] = count / self.total_word_count\n",
    "\n",
    "    return unigram_probabilities\n",
    "\n",
    "  def bigram_distribution(self, w1):\n",
    "    bigram_probabilities = {}\n",
    "    for w2, count in self.bigram_counts.items():\n",
    "      if w1 == w2[0]:\n",
    "        bigram_probabilities[w2[1]] = count / self.unigram_counts[w1]\n",
    "\n",
    "    return bigram_probabilities\n",
    "\n",
    "  def trigram_distribution(self, w1, w2):\n",
    "    trigram_probabilities = {}\n",
    "    for w3, count in self.trigram_counts.items():\n",
    "      if w1 == w3[0] and w2 == w3[1]:\n",
    "        trigram_probabilities[w3[2]] = count / self.bigram_counts[(w1, w2)]\n",
    "    return trigram_probabilities\n",
    "\n",
    "language_model = LanguageModel(tokenize_sentence)\n",
    "unigram_prob = language_model.unigram_distribution()\n",
    "bigram_prob = language_model.bigram_distribution(\"present\")\n",
    "trigram_prob = language_model.trigram_distribution(\"raw\",\"material\")\n",
    "print(bigram_prob)\n",
    "print(trigram_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212b0af-448a-43e0-af0b-04eb910b1df8",
   "metadata": {},
   "source": [
    "Task 1c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ffe4bd4-0a44-4adc-bc33-951edad9aa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in unigram model:  (47421,)\n",
      "Number of parameters in bigram model:  (383665, 2)\n",
      "Number of parameters in trigram model:  (689309, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters in unigram model: \",np.array( list(language_model.unigram_counts.keys())).shape)\n",
    "print(\"Number of parameters in bigram model: \",np.array( list(language_model.bigram_counts.keys())).shape)\n",
    "print(\"Number of parameters in trigram model: \",np.array( list(language_model.trigram_counts.keys())).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3667c1f-c023-42b6-a398-eb66bd8e3288",
   "metadata": {},
   "source": [
    "The number of parameters of the unigram, bigram and trigram distributions scales with the number of different words in the corpus:\n",
    "Unigram distribution: The unigram distribution has one parameter for each pair of words in the corpus. For a corpus with N different words, the bigram distribution has N parameters.\n",
    "Bigram distribution: The bigram distribution has one parameter for each pair of words in the corpus. For a corpus with N different words, the bigram distribution has N * N parameters.\n",
    "Trigram distribution: The trigram distribution has one parameter for each triple of words in the corpus. For a corpus with N different words, the trigram distribution has N * N * N parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d01a9-ef04-471c-bcb4-e2a66ae2b911",
   "metadata": {},
   "source": [
    "Task 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcb4d9b4-b38e-48dc-b6e8-7ee9a16b4769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she\n"
     ]
    }
   ],
   "source": [
    "def Sample(language_model, distribution, previous_word=\"<s>\"):\n",
    "  if distribution== \"unigram\":\n",
    "    unigram_prob = language_model.unigram_distribution()\n",
    "    sample =random.choices(list(unigram_prob.keys()), weights=list(unigram_prob.values()))[0]\n",
    "  elif distribution== \"bigram\":\n",
    "    previous_word= \"<s>\"\n",
    "    bigram_prob = language_model.bigram_distribution(previous_word)\n",
    "    sample = random.choices(list(bigram_prob.keys()), weights=list(bigram_prob.values()))[0]\n",
    "  elif distribution ==\"trigram\":\n",
    "    if previous_word == \"<s>\": previous_word = (\"<s>\", \"</s>\")\n",
    "    trigram_prob=language_model.trigram_distribution(previous_word[0], previous_word[1])\n",
    "    sample=random.choices(list(trigram_prob.keys()), weights=list(trigram_prob.values()))[0]\n",
    "  return sample\n",
    "\n",
    "# sample = sample(language_model, \"unigram\")\n",
    "# print(sample)\n",
    "sample = Sample(language_model, \"bigram\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ecf66-d6fa-468d-86d1-b942b70b7549",
   "metadata": {},
   "source": [
    "Task 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "954b38eb-1b61-4e7c-ab86-08767eff60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_unigram(language_model,max_sentance_length=100):\n",
    "      sentence = []\n",
    "      sentence_length=0\n",
    "      while True:\n",
    "        word = Sample(language_model, \"unigram\")\n",
    "        if word == \"</s>\" or sentence_length==max_sentance_length:\n",
    "          break\n",
    "        sentence.append(word)\n",
    "        sentence_length+=1\n",
    "      return \" \".join(sentence)\n",
    "\n",
    "\n",
    "def generate_sentence_bigram(language_model,max_sentance_length=100):\n",
    "      sentence = []\n",
    "      sentence_length=0\n",
    "      previous_word = \"<s>\"\n",
    "      while True:\n",
    "        word = Sample(language_model, \"bigram\", previous_word)\n",
    "        if word == \"</s>\" or sentence_length==max_sentance_length:\n",
    "          break\n",
    "        sentence.append(word)\n",
    "        previous_word = word\n",
    "        sentence_length+=1\n",
    "      return \" \".join(sentence)\n",
    "\n",
    "\n",
    "def generate_sentence_trigram(language_model,max_sentance_length=100):\n",
    "      sentence = []\n",
    "      sentence_length=0\n",
    "      previous_two_words = (\"<s>\", Sample(language_model, \"unigram\"))\n",
    "      while True:\n",
    "        word = Sample(language_model, \"trigram\", previous_two_words)\n",
    "        # if word == \"</s>\" or sentence_length==max_sentance_length:\n",
    "        if word == \"</s>\":\n",
    "          break\n",
    "        sentence.append(word)\n",
    "        previous_two_words = (previous_two_words[1], word)\n",
    "        sentence_length+=1\n",
    "      return \" \".join(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e689b-73e1-41f2-afee-a7c1a2621e66",
   "metadata": {},
   "source": [
    "Task 2c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e161d4f5-2557-46b5-9624-b5f33a2e8a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on the genuine brown by not found of of'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentence = generate_sentence_unigram(language_model)\n",
    "unigram_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92aa9c8-824d-4105-9e72-a35f1317c0e0",
   "metadata": {},
   "source": [
    "The unigram generator only considers the frequency of individual words, and it does not consider the relationships between words or order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "727709f3-df2c-46b4-90c5-455e2cebd89e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"what the drunk pete he most both let it the if notwithstanding in a the this the cady but we'll to at the because we success in that the to others so she mrs maybe he their he other the this st while when a i a only the and welsh when gone if the prednisone indeed as the by the the the can or now one a a but harris he by as by pels that the to another in he he it a to he and it styka we i there with instead mrs a frog-marched it in\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_sentence = generate_sentence_bigram(language_model,100)\n",
    "bigram_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da0e9cf-ee97-4a53-a61f-6926bbe913ee",
   "metadata": {},
   "source": [
    "Bigram generator only considers the frequency of pairs of words, and it does not consider the larger context of the sentence. Generator tend to generate very large text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e768144a-0a28-436f-9580-2b542a85f9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'were not however find himself forced into a faith which is easily installed'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_sentence = generate_sentence_trigram(language_model,100)\n",
    "trigram_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331fa40b-e39e-4c91-8808-117118799a12",
   "metadata": {},
   "source": [
    "This the most coherent of the three sentences. This is because the trigram generator considers the frequency of triples of words, which allows it to capture more of the context of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb5ddb-ca53-461f-9edb-bf4baa0f4840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
